{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis usando Deep Learning para espa√±ol en textos cortos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We work on a dataset of tweets labelled as positive or negative, more information about the dataset \n",
    "can be found on http://www.sepln.org/workshops/tass/. \n",
    "\n",
    "The dataset that we used can download here:\n",
    "\n",
    "- http://www.sepln.org/workshops/tass/2017/#datasets\n",
    "- http://www.sepln.org/workshops/tass/2018/#datasets\n",
    "\n",
    "Some description about this datasets:\n",
    "\n",
    "1. ----\n",
    "2. ----\n",
    "3. ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Getting train, validation and test subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dapal\\AppData\\Local\\conda\\conda\\envs\\SI\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import lib.xmlreader as xml\n",
    "import lib.utils as ut\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import gensim.models.word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs= xml.readXML(\"../database/TASS/TASS2017/task1-Training.xml\")\n",
    "test_docs= xml.readXML(\"../database/TASS/TASS2017/task1-Development.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets = []\n",
    "train_labels = []\n",
    "for doc in train_docs:\n",
    "    # train_tweets.append(ut.tokenize(doc.content, 0)['clean'])\n",
    "    train_tweets.append(doc.content)\n",
    "    train_labels.append(doc.polarity)\n",
    "\n",
    "test_tweets = []\n",
    "test_labels = []\n",
    "for doc in test_docs:\n",
    "    # test_tweets.append(ut.tokenize(doc.content, 0)['clean'])\n",
    "    test_tweets.append(doc.content)\n",
    "    test_labels.append(doc.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P1CvRJmH-buq"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = (pd.Series(train_tweets), pd.Series(test_tweets), pd.Series(train_labels), pd.Series(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l5B1Mf3Q-h-o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set has total 736 entries with 56.79% negative, 43.21% positive\n",
      "Test set has total 375 entries with 58.40% negative, 41.60% positive\n"
     ]
    }
   ],
   "source": [
    "fmt1_ = \"Train set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\"\n",
    "print(fmt1_.format(len(x_train),\n",
    "      (len(x_train[y_train == 0]) / (len(x_train)*1.))*100,\n",
    "      (len(x_train[y_train == 1]) / (len(x_train)*1.))*100))\n",
    "\n",
    "fmt3_ = \"Test set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\"\n",
    "print(fmt3_.format(len(x_test),\n",
    "      (len(x_test[y_test == 0]) / (len(x_test)*1.))*100,\n",
    "      (len(x_test[y_test == 1]) / (len(x_test)*1.))*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "def gensim_load_vec(path=\"../database/embeddings/SBW-vectors-300-min5.bin\"):\n",
    "    #use gensim_emb.wv.index2word if used this way to load vectors\n",
    "    #gensim_emb = gensim.models.word2vec.Word2Vec.load(path)\n",
    "    gensim_emb =  gensim.models.KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "    vocab = gensim_emb.index2word\n",
    "    vec = gensim_emb.syn0\n",
    "    shape = gensim_emb.syn0.shape\n",
    "    return gensim_emb, vec, shape, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dapal\\AppData\\Local\\conda\\conda\\envs\\SI\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \n",
      "C:\\Users\\dapal\\AppData\\Local\\conda\\conda\\envs\\SI\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "gensim_emb, vec, shape, vocab = gensim_load_vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = CountVectorizer(tokenizer=ut.tokenizer, min_df=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1111, 4227)\n"
     ]
    }
   ],
   "source": [
    "X = counter.fit_transform(train_tweets + test_tweets)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4227, 300)\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = X.shape[1]\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, 300))\n",
    "for word in list(counter.vocabulary_.keys()):\n",
    "    try:\n",
    "        i = counter.vocabulary_[word]\n",
    "        embedding_matrix[i] = gensim_emb[word]\n",
    "    except KeyError:\n",
    "        pass\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(embedding_matrix[counter.vocabulary_['hola']], gensim_emb['hola'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = []\n",
    "for tweet in train_tweets+test_tweets:\n",
    "    sentence = []\n",
    "    for word in ut.tokenizer(tweet):\n",
    "        try:\n",
    "            i = counter.vocabulary_[word]\n",
    "            sentence.append(i)\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "    sequences.append(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data train tensor: (736, 45)\n",
      "Shape of data val  tensor: (375, 45)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "x_train_seq = pad_sequences(sequences[:len(x_train)], maxlen=45)\n",
    "x_test_seq  = pad_sequences(sequences[-len(x_test):], maxlen=45)\n",
    "print('Shape of data train tensor:', x_train_seq.shape)\n",
    "print('Shape of data val  tensor:', x_test_seq.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 45)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 45, 300)      1268100     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 44, 100)      60100       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 43, 100)      90100       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 42, 100)      120100      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 100)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 100)          0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 100)          0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 300)          0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          77056       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            257         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1)            0           dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,615,713\n",
      "Trainable params: 1,615,713\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seed = 7\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "from keras.layers import Input, Dense, concatenate, Activation\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "tweet_input = Input(shape=(45,), dtype='int32')\n",
    "\n",
    "tweet_encoder = Embedding(VOCAB_SIZE, 300, weights=[embedding_matrix], input_length=45, trainable=True)(tweet_input)\n",
    "bigram_branch = Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "bigram_branch = GlobalMaxPooling1D()(bigram_branch)\n",
    "trigram_branch = Conv1D(filters=100, kernel_size=3, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "trigram_branch = GlobalMaxPooling1D()(trigram_branch)\n",
    "fourgram_branch = Conv1D(filters=100, kernel_size=4, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "fourgram_branch = GlobalMaxPooling1D()(fourgram_branch)\n",
    "merged = concatenate([bigram_branch, trigram_branch, fourgram_branch], axis=1)\n",
    "\n",
    "merged = Dense(256, activation='relu')(merged)\n",
    "merged = Dropout(0.2)(merged)\n",
    "merged = Dense(1)(merged)\n",
    "output = Activation('sigmoid')(merged)\n",
    "model = Model(inputs=[tweet_input], outputs=[output])\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 736 samples, validate on 375 samples\n",
      "Epoch 1/10\n",
      "736/736 [==============================] - 3s 4ms/step - loss: 0.7425 - acc: 0.4905 - val_loss: 0.7031 - val_acc: 0.5840\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.58400, saving model to model/CNN_best_weights.01-0.5840.hdf5\n",
      "Epoch 2/10\n",
      "736/736 [==============================] - 3s 4ms/step - loss: 0.6297 - acc: 0.6807 - val_loss: 0.6423 - val_acc: 0.6373\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.58400 to 0.63733, saving model to model/CNN_best_weights.02-0.6373.hdf5\n",
      "Epoch 3/10\n",
      "736/736 [==============================] - 2s 3ms/step - loss: 0.5159 - acc: 0.8098 - val_loss: 0.6078 - val_acc: 0.6773\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.63733 to 0.67733, saving model to model/CNN_best_weights.03-0.6773.hdf5\n",
      "Epoch 4/10\n",
      "736/736 [==============================] - 2s 3ms/step - loss: 0.3559 - acc: 0.9049 - val_loss: 0.5561 - val_acc: 0.7253\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.67733 to 0.72533, saving model to model/CNN_best_weights.04-0.7253.hdf5\n",
      "Epoch 5/10\n",
      "736/736 [==============================] - 2s 3ms/step - loss: 0.1900 - acc: 0.9674 - val_loss: 0.5282 - val_acc: 0.7653\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.72533 to 0.76533, saving model to model/CNN_best_weights.05-0.7653.hdf5\n",
      "Epoch 6/10\n",
      "736/736 [==============================] - 2s 3ms/step - loss: 0.0749 - acc: 0.9891 - val_loss: 0.5317 - val_acc: 0.7760\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.76533 to 0.77600, saving model to model/CNN_best_weights.06-0.7760.hdf5\n",
      "Epoch 7/10\n",
      "736/736 [==============================] - 2s 3ms/step - loss: 0.0246 - acc: 1.0000 - val_loss: 0.5933 - val_acc: 0.7440\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.77600\n",
      "Epoch 8/10\n",
      "736/736 [==============================] - 2s 3ms/step - loss: 0.0106 - acc: 1.0000 - val_loss: 0.5838 - val_acc: 0.7653\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.77600\n",
      "Epoch 9/10\n",
      "736/736 [==============================] - 2s 3ms/step - loss: 0.0052 - acc: 1.0000 - val_loss: 0.6041 - val_acc: 0.7680\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.77600\n",
      "Epoch 10/10\n",
      "736/736 [==============================] - 2s 3ms/step - loss: 0.0033 - acc: 1.0000 - val_loss: 0.6379 - val_acc: 0.7573\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.77600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2417e2baa20>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "filepath=\"model/CNN_best_weights.{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "model.fit(x_train_seq, y_train, batch_size=64, epochs=10,\n",
    "                     validation_data=(x_test_seq, y_test), callbacks = [checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 0s 542us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6379356830120086, 0.7573333315849304]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from keras.models import load_model\n",
    "\n",
    "#loaded_CNN_model = load_model('model/CNN_best_weights.05-0.5400.hdf5')\n",
    "#loaded_CNN_model.evaluate(x=x_val_seq, y=y_validation)\n",
    "model.evaluate(x=x_test_seq, y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'acc']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics_names"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cnn_word2vec.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
