{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis usando Deep Learning para espa√±ol en textos cortos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We work on a dataset of tweets labelled as positive or negative, more information about the dataset \n",
    "can be found on http://www.sepln.org/workshops/tass/. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Getting train, validation and test subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib.xmlreader as xml\n",
    "import lib.utils as ut\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import gensim.models.word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs= xml.readXML(\"../database/TASS/TASS2017/task1-Training.xml\")\n",
    "test_docs= xml.readXML(\"../database/TASS/TASS2017/task1-Development.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets = []\n",
    "train_labels = []\n",
    "for doc in train_docs:\n",
    "    # train_tweets.append(ut.tokenize(doc.content, 0)['clean'])\n",
    "    train_tweets.append(doc.content)\n",
    "    train_labels.append(doc.polarity)\n",
    "\n",
    "test_tweets = []\n",
    "test_labels = []\n",
    "for doc in test_docs:\n",
    "    # test_tweets.append(ut.tokenize(doc.content, 0)['clean'])\n",
    "    test_tweets.append(doc.content)\n",
    "    test_labels.append(doc.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_tweets), len(test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P1CvRJmH-buq"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = (pd.Series(train_tweets), pd.Series(test_tweets), pd.Series(train_labels), pd.Series(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l5B1Mf3Q-h-o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set has total 736 entries with 56.79% negative, 43.21% positive\n",
      "Test set has total 375 entries with 58.40% negative, 41.60% positive\n"
     ]
    }
   ],
   "source": [
    "fmt1_ = \"Train set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\"\n",
    "print(fmt1_.format(len(x_train),\n",
    "      (len(x_train[y_train == 0]) / (len(x_train)*1.))*100,\n",
    "      (len(x_train[y_train == 1]) / (len(x_train)*1.))*100))\n",
    "\n",
    "fmt3_ = \"Test set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\"\n",
    "print(fmt3_.format(len(x_test),\n",
    "      (len(x_test[y_test == 0]) / (len(x_test)*1.))*100,\n",
    "      (len(x_test[y_test == 1]) / (len(x_test)*1.))*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "def gensim_load_vec(path=\"../database/embeddings/SBW-vectors-300-min5.bin\"):\n",
    "    #use gensim_emb.wv.index2word if used this way to load vectors\n",
    "    #gensim_emb = gensim.models.word2vec.Word2Vec.load(path)\n",
    "    gensim_emb =  gensim.models.KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "    vocab = gensim_emb.index2word\n",
    "    vec = gensim_emb.syn0\n",
    "    shape = gensim_emb.syn0.shape\n",
    "    return gensim_emb, vec, shape, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jose/.local/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \n",
      "/home/jose/.local/lib/python3.6/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "gensim_emb, vec, shape, vocab = gensim_load_vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counter = CountVectorizer(strip_accents=\"unicode\", tokenizer=ut.tokenizer)\n",
    "counter = CountVectorizer(max_features=1240,tokenizer=ut.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1111, 4227)\n"
     ]
    }
   ],
   "source": [
    "X = counter.fit_transform(train_tweets + test_tweets)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4227, 300)\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = X.shape[1]\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, 300))\n",
    "for word in list(counter.vocabulary_.keys()):\n",
    "    try:\n",
    "        i = counter.vocabulary_[word]\n",
    "        embedding_matrix[i] = gensim_emb[word]\n",
    "    except KeyError:\n",
    "        pass\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(embedding_matrix[counter.vocabulary_['hola']], gensim_emb['hola'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = []\n",
    "for tweet in train_tweets+test_tweets:\n",
    "    sentence = []\n",
    "    for word in ut.tokenizer(tweet):\n",
    "        try:\n",
    "            i = counter.vocabulary_[word]\n",
    "            sentence.append(i)\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "    sequences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data train tensor: (736, 45)\n",
      "Shape of data val  tensor: (375, 45)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "x_train_seq = pad_sequences(sequences[:len(x_train)], maxlen=45)\n",
    "x_test_seq  = pad_sequences(sequences[-len(x_test):], maxlen=45)\n",
    "print('Shape of data train tensor:', x_train_seq.shape)\n",
    "print('Shape of data val  tensor:', x_test_seq.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/job:localhost/replica:0/task:0/device:GPU:0']\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_20 (InputLayer)           (None, 45)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_20 (Embedding)        (None, 45, 300)      1268100     input_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, 44, 20)       12020       embedding_20[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, 43, 20)       18020       embedding_20[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, 42, 20)       24020       embedding_20[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_53 (Global (None, 20)           0           conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_54 (Global (None, 20)           0           conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_55 (Global (None, 20)           0           conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 60)           0           global_max_pooling1d_53[0][0]    \n",
      "                                                                 global_max_pooling1d_54[0][0]    \n",
      "                                                                 global_max_pooling1d_55[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_38 (Dense)                (None, 128)          7808        concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 128)          0           dense_38[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_39 (Dense)                (None, 1)            129         dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 1)            0           dense_39[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,330,097\n",
      "Trainable params: 1,330,097\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras \n",
    "\n",
    "from keras import backend as K\n",
    "print(K.tensorflow_backend._get_available_gpus())\n",
    "\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=4, \\\n",
    "                        inter_op_parallelism_threads=4, \\\n",
    "                        allow_soft_placement=True,\\\n",
    "                        device_count = {'CPU' : 1, 'GPU' : 0})\n",
    "\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)\n",
    "    \n",
    "seed = 7\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "from keras.layers import Input, Dense, concatenate, Activation\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "tweet_input = Input(shape=(45,), dtype='int32')\n",
    "\n",
    "tweet_encoder = Embedding(VOCAB_SIZE, 300, weights=[embedding_matrix], input_length=45, trainable=True)(tweet_input)\n",
    "bigram_branch = Conv1D(filters=20, kernel_size=2, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "bigram_branch  = GlobalMaxPooling1D()(bigram_branch)\n",
    "trigram_branch = Conv1D(filters=20, kernel_size=3, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "trigram_branch  = GlobalMaxPooling1D()(trigram_branch)\n",
    "fourgram_branch = Conv1D(filters=20, kernel_size=4, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "fourgram_branch = GlobalMaxPooling1D()(fourgram_branch)\n",
    "merged = concatenate([bigram_branch, trigram_branch, fourgram_branch], axis=1)\n",
    "\n",
    "merged = Dense(128, activation='relu')(merged)\n",
    "merged = Dropout(0.5)(merged)\n",
    "\n",
    "merged = Dense(1)(merged)\n",
    "output = Activation('sigmoid')(merged)\n",
    "model = Model(inputs=[tweet_input], outputs=[output])\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 736 samples, validate on 375 samples\n",
      "Epoch 1/10\n",
      "736/736 [==============================] - 2s 3ms/step - loss: 0.6938 - acc: 0.5598 - val_loss: 0.6760 - val_acc: 0.6400\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.64000, saving model to model/CNN_best_weights.01-0.6400.hdf5\n",
      "Epoch 2/10\n",
      "736/736 [==============================] - 1s 1ms/step - loss: 0.6381 - acc: 0.6359 - val_loss: 0.6465 - val_acc: 0.6160\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.64000\n",
      "Epoch 3/10\n",
      "736/736 [==============================] - 1s 1ms/step - loss: 0.5461 - acc: 0.7704 - val_loss: 0.6244 - val_acc: 0.6933\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.64000 to 0.69333, saving model to model/CNN_best_weights.03-0.6933.hdf5\n",
      "Epoch 4/10\n",
      "736/736 [==============================] - 1s 1ms/step - loss: 0.4379 - acc: 0.8750 - val_loss: 0.5801 - val_acc: 0.7173\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.69333 to 0.71733, saving model to model/CNN_best_weights.04-0.7173.hdf5\n",
      "Epoch 5/10\n",
      "736/736 [==============================] - 1s 1ms/step - loss: 0.3035 - acc: 0.9375 - val_loss: 0.5860 - val_acc: 0.6960\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.71733\n",
      "Epoch 6/10\n",
      "736/736 [==============================] - 1s 1ms/step - loss: 0.2001 - acc: 0.9606 - val_loss: 0.5513 - val_acc: 0.7280\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.71733 to 0.72800, saving model to model/CNN_best_weights.06-0.7280.hdf5\n",
      "Epoch 7/10\n",
      "736/736 [==============================] - 1s 1ms/step - loss: 0.1002 - acc: 0.9891 - val_loss: 0.5492 - val_acc: 0.7253\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.72800\n",
      "Epoch 8/10\n",
      "736/736 [==============================] - 1s 1ms/step - loss: 0.0563 - acc: 0.9973 - val_loss: 0.5700 - val_acc: 0.7200\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.72800\n",
      "Epoch 9/10\n",
      "736/736 [==============================] - 1s 1ms/step - loss: 0.0309 - acc: 0.9986 - val_loss: 0.5799 - val_acc: 0.7387\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.72800 to 0.73867, saving model to model/CNN_best_weights.09-0.7387.hdf5\n",
      "Epoch 10/10\n",
      "736/736 [==============================] - 1s 1ms/step - loss: 0.0201 - acc: 1.0000 - val_loss: 0.5951 - val_acc: 0.7307\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.73867\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbb6596bc18>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "filepath=\"model/CNN_best_weights.{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "model.fit(x_train_seq, y_train, batch_size=64, epochs=10,\n",
    "                     validation_data=(x_test_seq, y_test), callbacks = [checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 0s 176us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5950926593144734, 0.7306666649182637]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from keras.models import load_model\n",
    "\n",
    "#loaded_CNN_model = load_model('model/CNN_best_weights.05-0.5400.hdf5')\n",
    "#loaded_CNN_model.evaluate(x=x_val_seq, y=y_validation)\n",
    "model.evaluate(x=x_test_seq, y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'acc']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cnn_word2vec.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
