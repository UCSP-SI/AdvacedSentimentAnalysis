{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib.xmlreader as xml\n",
    "import lib.utils as ut\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import gensim\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs  = xml.readXML(\"../database/TASS/TASS2018/task1-Training.xml\"   ,[0,1,2,3])\n",
    "dev_docs    = xml.readXML(\"../database/TASS/TASS2018/task1-Development.xml\",[0,1,2,3])\n",
    "test_docs   = xml.readXML(\"../database/TASS/TASS2018/task1-Test.xml\"       ,[0,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert (len(train_docs) == 1008)\n",
    "#assert (len(dev_docs)   ==  506)\n",
    "#assert (len(test_docs)  == 1899)\n",
    "\n",
    "assert (len(train_docs) == 1000)\n",
    "assert (len(dev_docs)   ==  500)\n",
    "assert (len(test_docs)  == 1428)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = []\n",
    "for train_doc in train_docs:\n",
    "    train_labels.append(train_doc.polarity)\n",
    "    \n",
    "dev_labels   = []\n",
    "for dev_doc in dev_docs:\n",
    "    dev_labels.append(dev_doc.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSI_train_docs = [train_docs[i] for i in range(len(train_labels)) if train_labels[i] == 0]\n",
    "NEGA_train_docs = [train_docs[i] for i in range(len(train_labels)) if train_labels[i] == 1]\n",
    "NEUT_train_docs = [train_docs[i] for i in range(len(train_labels)) if train_labels[i] == 2]\n",
    "NONE_train_docs = [train_docs[i] for i in range(len(train_labels)) if train_labels[i] == 3]\n",
    "\n",
    "level_train_docs = [POSI_train_docs,NEGA_train_docs,NEUT_train_docs,NONE_train_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Sentences = 242\n",
      "       \r",
      "Negative Sentences = 231\n",
      "       \r",
      "Neutral  Sentences = 166\n",
      "       \r",
      "None Values        = 361\n"
     ]
    }
   ],
   "source": [
    "fmt = \"\"\"Positive Sentences = {:d}\n",
    "       \\rNegative Sentences = {:d}\n",
    "       \\rNeutral  Sentences = {:d}\n",
    "       \\rNone Values        = {:d}\"\"\"\n",
    "\n",
    "print(fmt.format(len(POSI_train_docs),\n",
    "                 len(NEGA_train_docs),\n",
    "                 len(NEUT_train_docs),\n",
    "                 len(NONE_train_docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum number of sentences per level :  166\n"
     ]
    }
   ],
   "source": [
    "minSentLvl = min(len(POSI_train_docs),len(NEGA_train_docs),len(NEUT_train_docs),len(NONE_train_docs))\n",
    "\n",
    "print('Minimum number of sentences per level : ', minSentLvl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_docs = []\n",
    "for i in range(len(level_train_docs)):\n",
    "    level_per = random.sample(level_train_docs[i],len(level_train_docs[i]))\n",
    "    new_train_docs.append(level_per[:minSentLvl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New size of sentences:\n",
      "\n",
      "Positive Sentences = 166\n",
      "       \r",
      "Negative Sentences = 166\n",
      "       \r",
      "Neutral  Sentences = 166\n",
      "       \r",
      "None Values        = 166\n"
     ]
    }
   ],
   "source": [
    "print(\"New size of sentences:\\n\")\n",
    "fmt = \"\"\"Positive Sentences = {:d}\n",
    "       \\rNegative Sentences = {:d}\n",
    "       \\rNeutral  Sentences = {:d}\n",
    "       \\rNone Values        = {:d}\"\"\"\n",
    "\n",
    "print(fmt.format(len(new_train_docs[0]),\n",
    "                 len(new_train_docs[1]),\n",
    "                 len(new_train_docs[2]),\n",
    "                 len(new_train_docs[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuf_train_docs size =  664\n"
     ]
    }
   ],
   "source": [
    "flat_train_docs = [item for sublist in new_train_docs for item in sublist]\n",
    "shuf_train_docs = random.sample(flat_train_docs,len(flat_train_docs))\n",
    "\n",
    "assert (len(shuf_train_docs) == 4 * minSentLvl)\n",
    "print(\"shuf_train_docs size = \", len(shuf_train_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for doc in shuf_train_docs + dev_docs + test_docs:\n",
    "    corpus.append(doc.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences =  2592\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentences = \", (len(test_docs + dev_docs + shuf_train_docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuf_train_labels = []\n",
    "for doc in shuf_train_docs:\n",
    "    shuf_train_labels.append(doc.polarity)\n",
    "    \n",
    "assert (len(shuf_train_labels) == len(shuf_train_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gensim_load_vec(path=\"../database/embeddings/SBW-vectors-300-min5.bin\"):\n",
    "    gensim_emb =  gensim.models.KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "    vocab = gensim_emb.index2word\n",
    "    vec = gensim_emb.syn0\n",
    "    shape = gensim_emb.syn0.shape\n",
    "    return gensim_emb, vec, shape, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jose/.local/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  after removing the cwd from sys.path.\n",
      "/home/jose/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "gensim_emb, vec, shape, vocab = gensim_load_vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = CountVectorizer(tokenizer=ut.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2592, 8026)\n"
     ]
    }
   ],
   "source": [
    "X = counter.fit_transform(corpus)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "VOCAB_SIZE = X.shape[1]\n",
    "\n",
    "caption_texts = corpus\n",
    "Xc = counter.fit_transform(caption_texts).todense().astype(\"float\")\n",
    "print(Xc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = X.shape[1]\n",
    "\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, 300), np.float)\n",
    "\n",
    "for word in list(counter.vocabulary_.keys()):\n",
    "    try:\n",
    "        i = counter.vocabulary_[word]\n",
    "        embedding_matrix[i] = gensim_emb[word]\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(embedding_matrix[counter.vocabulary_['hola']], gensim_emb['hola'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets = []\n",
    "train_labels = shuf_train_labels\n",
    "for doc in shuf_train_docs:\n",
    "    train_tweets.append(doc.content)\n",
    "\n",
    "dev_tweets = []\n",
    "for doc in dev_docs:\n",
    "    dev_tweets.append(doc.content)\n",
    "\n",
    "test_tweets = []\n",
    "for doc in test_docs:\n",
    "    test_tweets.append(doc.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "ls = []\n",
    "sequences = []\n",
    "for tweet in train_tweets + dev_tweets + test_tweets:\n",
    "    sentence = []\n",
    "    for word in ut.tokenizer(tweet):\n",
    "        try:\n",
    "            i = counter.vocabulary_[word]\n",
    "            sentence.append(i)\n",
    "        except KeyError:\n",
    "            pass\n",
    "    \n",
    "    sequences.append(sentence)\n",
    "    ls.append(len(sentence))\n",
    "    \n",
    "MAXLEN = max(ls)\n",
    "print(MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXLEN = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data train tensor: (664, 30)\n",
      "Shape of data val  tensor : (500, 30)\n",
      "Shape of data test  tensor: (1428, 30)\n",
      "Shape of data train labels: 664\n",
      "Shape of data val   labels: 500\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "x_train_seq = pad_sequences(sequences[:len(train_tweets)], maxlen = MAXLEN)\n",
    "x_dev_seq   = pad_sequences(sequences[ len(train_tweets): len(train_tweets) + len(dev_tweets)], maxlen=MAXLEN)\n",
    "x_test_seq  = pad_sequences(sequences[-len(test_tweets):], maxlen=MAXLEN)\n",
    "\n",
    "print('Shape of data train tensor:', x_train_seq.shape)\n",
    "print('Shape of data val  tensor :', x_dev_seq.shape)\n",
    "print('Shape of data test  tensor:', x_test_seq.shape)\n",
    "\n",
    "print('Shape of data train labels:', len(train_labels))\n",
    "print('Shape of data val   labels:', len(dev_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/job:localhost/replica:0/task:0/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras \n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "from keras.layers import Input, Dense, concatenate, Activation\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "print(K.tensorflow_backend._get_available_gpus())\n",
    "\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=4, \\\n",
    "                        inter_op_parallelism_threads=4, \\\n",
    "                        allow_soft_placement=True,\\\n",
    "                        device_count = {'CPU' : 1, 'GPU' : 0})\n",
    "\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)\n",
    "    \n",
    "seed = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 30, 300)      2407800     input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 29, 100)      60100       embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 28, 100)      90100       embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 27, 100)      120100      embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_16 (Global (None, 100)          0           conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_17 (Global (None, 100)          0           conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_18 (Global (None, 100)          0           conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 300)          0           global_max_pooling1d_16[0][0]    \n",
      "                                                                 global_max_pooling1d_17[0][0]    \n",
      "                                                                 global_max_pooling1d_18[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 32)           9632        concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 32)           0           dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 4)            132         dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 4)            0           dense_12[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 2,687,864\n",
      "Trainable params: 280,064\n",
      "Non-trainable params: 2,407,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tweet_input     = Input(shape=(MAXLEN,), dtype='int32')\n",
    "\n",
    "tweet_encoder   = Embedding(VOCAB_SIZE, 300, weights=[embedding_matrix], input_length=MAXLEN, trainable=False)(tweet_input)\n",
    "\n",
    "bigram_branch   = Conv1D(filters=100,kernel_size=2, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "bigram_branch   = GlobalMaxPooling1D()(bigram_branch)\n",
    "\n",
    "trigram_branch  = Conv1D(filters=100, kernel_size=3, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "trigram_branch  = GlobalMaxPooling1D()(trigram_branch)\n",
    "\n",
    "fourgram_branch = Conv1D(filters=100, kernel_size=4, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "fourgram_branch = GlobalMaxPooling1D()(fourgram_branch)\n",
    "\n",
    "merged = concatenate([bigram_branch, trigram_branch, fourgram_branch], axis=1)\n",
    "merged = Dense(32, activation='relu')(merged)\n",
    "merged = Dropout(0.5)(merged)\n",
    "\n",
    "merged = Dense(4)(merged)\n",
    "output = Activation('softmax')(merged)\n",
    "model  = Model(inputs=[tweet_input], outputs=[output])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 664 samples, validate on 500 samples\n",
      "Epoch 1/10\n",
      "664/664 [==============================] - 1s 2ms/step - loss: 1.4197 - acc: 0.2681 - val_loss: 1.3646 - val_acc: 0.4660\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.46600, saving model to model/CNN_best_weights_CNN.01-0.4660.hdf5\n",
      "Epoch 2/10\n",
      "664/664 [==============================] - 1s 873us/step - loss: 1.3749 - acc: 0.2952 - val_loss: 1.3627 - val_acc: 0.2420\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.46600\n",
      "Epoch 3/10\n",
      "664/664 [==============================] - 1s 895us/step - loss: 1.3601 - acc: 0.3012 - val_loss: 1.3671 - val_acc: 0.4620\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.46600\n",
      "Epoch 4/10\n",
      "664/664 [==============================] - 1s 909us/step - loss: 1.3411 - acc: 0.3795 - val_loss: 1.3492 - val_acc: 0.3260\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.46600\n",
      "Epoch 5/10\n",
      "664/664 [==============================] - 1s 971us/step - loss: 1.3022 - acc: 0.3539 - val_loss: 1.3290 - val_acc: 0.3900\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.46600\n",
      "Epoch 6/10\n",
      "664/664 [==============================] - 1s 898us/step - loss: 1.2643 - acc: 0.4187 - val_loss: 1.2981 - val_acc: 0.4400\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.46600\n",
      "Epoch 7/10\n",
      "664/664 [==============================] - 1s 978us/step - loss: 1.2271 - acc: 0.4232 - val_loss: 1.3046 - val_acc: 0.4500\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.46600\n",
      "Epoch 8/10\n",
      "664/664 [==============================] - 1s 931us/step - loss: 1.1766 - acc: 0.4383 - val_loss: 1.3129 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.46600\n",
      "Epoch 9/10\n",
      "664/664 [==============================] - 1s 989us/step - loss: 1.1393 - acc: 0.4759 - val_loss: 1.2845 - val_acc: 0.4560\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.46600\n",
      "Epoch 10/10\n",
      "664/664 [==============================] - 1s 915us/step - loss: 1.1054 - acc: 0.5166 - val_loss: 1.3084 - val_acc: 0.4240\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.46600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc046f705c0>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath=\"model/CNN_best_weights_CNN.{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "model.fit(x_train_seq, to_categorical(train_labels), batch_size=64, epochs=10,\n",
    "                     validation_data=(x_dev_seq, to_categorical(dev_labels)), callbacks = [checkpoint], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "best_model = load_model('model/CNN_best_weights_CNN.01-0.4660.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_values = np.argmax(best_model.predict(x_test_seq), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1428"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabel(num):\n",
    "    if num == 0:\n",
    "        return 'N'\n",
    "    elif num == 1:\n",
    "        return 'P'\n",
    "    elif num == 2:\n",
    "        return 'NEU'\n",
    "    elif num == 3:\n",
    "        return 'NONE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def putTestValue(xmlFIle, out):\n",
    "    tree = ET.parse(xmlFIle)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    tweets = []\n",
    "    file = open(out,\"w\") \n",
    "    print(len(test_values))\n",
    "    for i,tweet in enumerate(root.iter('tweet')): \n",
    "        #print(i)\n",
    "        val = getLabel(test_values[i])\n",
    "        ID = tweet.find('tweetid').text\n",
    "        file.write(ID + \"\\t\" + val + \"\\n\")\n",
    "    file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1428\n"
     ]
    }
   ],
   "source": [
    "putTestValue(\"../database/TASS/TASS2018/task1-Test.xml\", \"output2018.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
