{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis usando Deep Learning para espa√±ol en textos cortos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We work on a dataset of tweets labelled as positive or negative, more information about the dataset \n",
    "can be found on http://www.sepln.org/workshops/tass/. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Getting train, validation and test subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dapal\\AppData\\Local\\conda\\conda\\envs\\SI\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import lib.xmlreader as xml\n",
    "import lib.utils as ut\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import gensim.models.word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs = xml.readXML(\"../database/TASS/TASS2018/task1-Training.xml\"  ,[0,1,2,3])\n",
    "test_docs  = xml.readXML(\"../database/TASS/TASS2018/task1-Development.xml\",[0,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets = []\n",
    "train_labels = []\n",
    "for doc in train_docs:\n",
    "    train_tweets.append(doc.content)\n",
    "    train_labels.append(doc.polarity)\n",
    "\n",
    "test_tweets = []\n",
    "test_labels = []\n",
    "for doc in test_docs:\n",
    "    test_tweets.append(doc.content)\n",
    "    test_labels.append(doc.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 500)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_tweets), len(test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minL = min(min(test_labels), min(train_labels))\n",
    "minL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = [tl - minL for tl in train_labels]\n",
    "test_labels = [tl - minL for tl in test_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxL = max(max(test_labels), max(train_labels))\n",
    "maxL == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P1CvRJmH-buq"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = (pd.Series(train_tweets), pd.Series(test_tweets), pd.Series(train_labels), pd.Series(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l5B1Mf3Q-h-o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set has total 1000 entries with 24.20% negative, 23.10% positive, 16.60% neutral, 36.10% NONE\n",
      "Test  set has total 500  entries with 21.20% negative, 19.00% positive, 12.20% neutral, 47.60% NONE\n"
     ]
    }
   ],
   "source": [
    "fmt1_ = \"Train set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive, {3:.2f}% neutral, {4:.2f}% NONE\"\n",
    "print(fmt1_.format(len(x_train),\n",
    "      (len(x_train[y_train == 0]) / (len(x_train)*1.))*100,\n",
    "      (len(x_train[y_train == 1]) / (len(x_train)*1.))*100,\n",
    "      (len(x_train[y_train == 2]) / (len(x_train)*1.))*100,\n",
    "      (len(x_train[y_train == 3]) / (len(x_train)*1.))*100))\n",
    "\n",
    "fmt3_ = \"Test  set has total {0}  entries with {1:.2f}% negative, {2:.2f}% positive, {3:.2f}% neutral, {4:.2f}% NONE\"\n",
    "print(fmt3_.format(len(x_test),\n",
    "      (len(x_test[y_test == 0]) / (len(x_test)*1.))*100,\n",
    "      (len(x_test[y_test == 1]) / (len(x_test)*1.))*100,\n",
    "      (len(x_test[y_test == 2]) / (len(x_test)*1.))*100,\n",
    "      (len(x_test[y_test == 3]) / (len(x_test)*1.))*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from gensim.models import KeyedVectors\n",
    "\n",
    "def gensim_load_vec(path=\"../database/embeddings/SBW-vectors-300-min5.bin\"):\n",
    "    #use gensim_emb.wv.index2word if used this way to load vectors\n",
    "    #gensim_emb = gensim.models.word2vec.Word2Vec.load(path)\n",
    "    gensim_emb =  gensim.models.KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "    vocab = gensim_emb.index2word\n",
    "    vec = gensim_emb.syn0\n",
    "    shape = gensim_emb.syn0.shape\n",
    "    return gensim_emb, vec, shape, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dapal\\AppData\\Local\\conda\\conda\\envs\\SI\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \n",
      "C:\\Users\\dapal\\AppData\\Local\\conda\\conda\\envs\\SI\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "gensim_emb, vec, shape, vocab = gensim_load_vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counter = CountVectorizer(tokenizer=ut.tokenizer, max_features=2999)\n",
    "#X = counter.fit_transform(train_tweets + test_tweets)\n",
    "#print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "#joblib.dump(counter, './counter.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = joblib.load('./counter.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert(X.shape[0] == (len(train_tweets) + len(test_tweets)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_mean = np.mean(embedding_matrix, axis=0)\n",
    "emb_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(counter.vocabulary_.keys())\n",
    "#2999 in counter.vocabulary_.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(counter.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 300)\n"
     ]
    }
   ],
   "source": [
    "#VOCAB_SIZE = X.shape[1] + 1\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, 300))\n",
    "cw = 0\n",
    "for ie in range(VOCAB_SIZE):\n",
    "    embedding_matrix[ie] = emb_mean\n",
    "for word in list(counter.vocabulary_.keys()):\n",
    "    try:\n",
    "        i = counter.vocabulary_[word]\n",
    "        embedding_matrix[i+1] = gensim_emb[word]\n",
    "    except KeyError:\n",
    "        cw+=1\n",
    "        pass\n",
    "\n",
    "np.save('emb.npy',embedding_matrix)\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 300)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.load('emb.npy')\n",
    "VOCAB_SIZE = embedding_matrix.shape[0]\n",
    "\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(embedding_matrix[counter.vocabulary_['hola']+1], gensim_emb['hola'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ut.tokenizer(train_tweets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "ls = []\n",
    "sequences = []\n",
    "for tweet in train_tweets+test_tweets:\n",
    "    sentence = []\n",
    "    for word in ut.tokenizer(tweet):\n",
    "        try:\n",
    "            i = counter.vocabulary_[word]\n",
    "            sentence.append(i+1)\n",
    "        except KeyError:\n",
    "            sentence.append(0)\n",
    "    \n",
    "    sequences.append(sentence)\n",
    "    ls.append(len(sentence))\n",
    "    \n",
    "MAXLEN = max(ls)\n",
    "print(MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data train tensor: (1000, 30)\n",
      "Shape of data val  tensor: (500, 30)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "x_train_seq = pad_sequences(sequences[:len(x_train)], maxlen=MAXLEN)\n",
    "x_test_seq  = pad_sequences(sequences[-len(x_test):], maxlen=MAXLEN)\n",

    "print('Shape of data train tensor:', x_train_seq.shape)\n",
    "print('Shape of data val  tensor:', x_test_seq.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/job:localhost/replica:0/task:0/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras \n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "from keras.layers import Input, Dense, concatenate, Activation\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "print(K.tensorflow_backend._get_available_gpus())\n",
    "\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=4, \\\n",
    "                        inter_op_parallelism_threads=4, \\\n",
    "                        allow_soft_placement=True,\\\n",
    "                        device_count = {'CPU' : 1, 'GPU' : 0})\n",
    "\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)\n",
    "    \n",
    "seed = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_35 (InputLayer)           (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_23 (Embedding)        (None, 30, 300)      900000      input_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_67 (Conv1D)              (None, 29, 200)      120200      embedding_23[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_68 (Conv1D)              (None, 28, 200)      180200      embedding_23[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_69 (Conv1D)              (None, 27, 200)      240200      embedding_23[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_67 (Global (None, 200)          0           conv1d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_68 (Global (None, 200)          0           conv1d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_69 (Global (None, 200)          0           conv1d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_55 (Concatenate)    (None, 600)          0           global_max_pooling1d_67[0][0]    \n",
      "                                                                 global_max_pooling1d_68[0][0]    \n",
      "                                                                 global_max_pooling1d_69[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_140 (Dense)               (None, 128)          76928       concatenate_55[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_67 (Dropout)            (None, 128)          0           dense_140[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_141 (Dense)               (None, 2)            258         dropout_67[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 2)            0           dense_141[0][0]                  \n",

      "==================================================================================================\n",
      "Total params: 1,517,786\n",
      "Trainable params: 1,517,786\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tweet_input     = Input(shape=(MAXLEN,), dtype='int32')\n",
    "\n",
    "tweet_encoder   = Embedding(VOCAB_SIZE, 300, weights=[embedding_matrix], input_length=MAXLEN, trainable=True)(tweet_input)\n",
    "bigram_branch   = Conv1D(filters=200,kernel_size=2, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "bigram_branch   = GlobalMaxPooling1D()(bigram_branch)\n",
    "trigram_branch  = Conv1D(filters=200, kernel_size=3, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "trigram_branch  = GlobalMaxPooling1D()(trigram_branch)\n",
    "fourgram_branch = Conv1D(filters=200, kernel_size=4, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "fourgram_branch = GlobalMaxPooling1D()(fourgram_branch)\n",
    "\n",
    "merged = concatenate([bigram_branch, trigram_branch, fourgram_branch], axis=1)\n",
    "merged = Dense(128, activation='relu')(merged)\n",
    "merged = Dropout(0.5)(merged)\n",
    "\n",
    "merged = Dense(2)(merged)\n",
    "output = Activation('softmax')(merged)\n",
    "model  = Model(inputs=[tweet_input], outputs=[output])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 744,

   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 527 samples, validate on 299 samples\n",
      "Epoch 1/10\n",
      "527/527 [==============================] - 5s 10ms/step - loss: 0.7410 - acc: 0.5920 - val_loss: 0.5130 - val_acc: 0.7960\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.79599, saving model to model/CNN_best_weights_NENO.01-0.7960.hdf5\n",
      "Epoch 2/10\n",
      "527/527 [==============================] - 1s 2ms/step - loss: 0.5939 - acc: 0.6869 - val_loss: 0.5292 - val_acc: 0.7960\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.79599\n",
      "Epoch 3/10\n",
      "527/527 [==============================] - 1s 2ms/step - loss: 0.5496 - acc: 0.7135 - val_loss: 0.5243 - val_acc: 0.7960\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.79599\n",
      "Epoch 4/10\n",
      "527/527 [==============================] - 1s 2ms/step - loss: 0.4869 - acc: 0.7571 - val_loss: 0.6630 - val_acc: 0.5886\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.79599\n",
      "Epoch 5/10\n",
      "527/527 [==============================] - 1s 2ms/step - loss: 0.4692 - acc: 0.8083 - val_loss: 0.5001 - val_acc: 0.7960\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.79599\n",
      "Epoch 6/10\n",
      "527/527 [==============================] - 1s 2ms/step - loss: 0.3708 - acc: 0.8691 - val_loss: 0.4902 - val_acc: 0.7960\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.79599\n",
      "Epoch 7/10\n",
      "527/527 [==============================] - 1s 2ms/step - loss: 0.2886 - acc: 0.9127 - val_loss: 0.5526 - val_acc: 0.7525\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.79599\n",
      "Epoch 8/10\n",
      "527/527 [==============================] - 1s 2ms/step - loss: 0.1859 - acc: 0.9658 - val_loss: 0.4965 - val_acc: 0.7826\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.79599\n",
      "Epoch 9/10\n",
      "527/527 [==============================] - 1s 2ms/step - loss: 0.1262 - acc: 0.9886 - val_loss: 0.5194 - val_acc: 0.7659\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.79599\n",
      "Epoch 10/10\n",
      "527/527 [==============================] - 1s 2ms/step - loss: 0.0842 - acc: 0.9981 - val_loss: 0.5636 - val_acc: 0.7592\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.79599\n"

     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0b9342ec50>"
      ]
     },
     "execution_count": 744,

     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath=\"model/CNN_best_weights_NENO.{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "model.fit(x_train_seq, to_categorical(y_train), batch_size=64, epochs=10,\n",
    "                     validation_data=(x_test_seq, to_categorical(y_test)), callbacks = [checkpoint], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299/299 [==============================] - 0s 554us/step\n"

     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.624254456531244, 0.6688963218676207]"
      ]
     },
     "execution_count": 423,

     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x=x_test_seq, y=to_categorical(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_input_ex = Input(shape=(MAXLEN,), dtype='int32')\n",
    "\n",
    "tweet_encoder = Embedding(VOCAB_SIZE, 300, weights=[embedding_matrix], input_length=MAXLEN, trainable=True)(tweet_input_ex)\n",
    "bigram_branch = Conv1D(filters=200,kernel_size=2, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "bigram_branch  = GlobalMaxPooling1D()(bigram_branch)\n",
    "trigram_branch = Conv1D(filters=200, kernel_size=3, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "trigram_branch  = GlobalMaxPooling1D()(trigram_branch)\n",
    "fourgram_branch = Conv1D(filters=200, kernel_size=4, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "fourgram_branch = GlobalMaxPooling1D()(fourgram_branch)\n",
    "merged = concatenate([bigram_branch, trigram_branch, fourgram_branch], axis=1)\n",
    "merged = Dense(128, activation='relu')(merged)\n",
    "merged = Dropout(0.5)(merged)\n",
    "\n",
    "merged = Dense(2)(merged)\n",
    "output_ex = Activation('softmax')(merged)\n"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "tweet_merged_input = Input(shape=(MAXLEN,), dtype='int32')\n",
    "\n",
    "model1      = load_model('model/CNN_best_weights_NEPO.07-0.7612.hdf5')\n",
    "output_NEPO = model1(tweet_merged_input)\n",
    "\n",
    "model2      = Model(inputs=[tweet_input_ex], outputs=[output_ex])\n",
    "output_NENO = model2(tweet_merged_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,

   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_106 (Model)               (None, 2)            1517786     input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "model_2 (Model)                 (None, 2)            1517786     input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 4)            0           model_106[1][0]                  \n",
      "                                                                 model_2[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 16)           80          concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 16)           0           dense_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, 16)           272         dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 16)           0           dense_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_32 (Dense)                (None, 16)           272         dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_33 (Dense)                (None, 4)            68          dense_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 4)            0           dense_33[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 3,036,264\n",
      "Trainable params: 3,036,264\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pred_merged    = concatenate([output_NEPO, output_NENO], axis=1)\n",
    "cross          = Dense(16, activation='relu')(pred_merged)\n",
    "cross          = Dropout(0.2)(cross)\n",
    "cross          = Dense(16, activation='relu')(cross)\n",
    "cross          = Dropout(0.2)(cross)\n",
    "cross          = Dense(16, activation='relu')(cross)\n",
    "\n",
    "cross          = Dense(maxL + 1)(cross)\n",
    "cross_output   = Activation('softmax')(cross)\n",
    "\n",
    "model_merged   = Model(inputs=[tweet_merged_input], outputs=[cross_output])\n",
    "model_merged.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "model_merged.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model_merged.to_json()\n",
    "with open(\"model/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 500 samples\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 1.3779 - acc: 0.3240 - val_loss: 1.3495 - val_acc: 0.4760\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.47600, saving model to model/CNN_best_weights_MERGED.01-0.4760.hdf5\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 1.3634 - acc: 0.3580 - val_loss: 1.3151 - val_acc: 0.4760\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.47600\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 1.3393 - acc: 0.3480 - val_loss: 1.2644 - val_acc: 0.4760\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.47600\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 1.2747 - acc: 0.3600 - val_loss: 1.2625 - val_acc: 0.4760\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.47600\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 1.1943 - acc: 0.3580 - val_loss: 1.2666 - val_acc: 0.4760\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.47600\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 1.0804 - acc: 0.4400 - val_loss: 1.2810 - val_acc: 0.4520\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.47600\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 1.0026 - acc: 0.5360 - val_loss: 1.3515 - val_acc: 0.4480\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.47600\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.9429 - acc: 0.5680 - val_loss: 1.3520 - val_acc: 0.3160\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.47600\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.9152 - acc: 0.5930 - val_loss: 1.6355 - val_acc: 0.4420\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.47600\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.8970 - acc: 0.6020 - val_loss: 1.9134 - val_acc: 0.4520\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.47600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe90b7044a8>"
      ]
     }
     "execution_count": 68,

     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath=\"model/CNN_best_weights_MERGED.{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "model_merged.fit(x_train_seq, to_categorical(y_train), batch_size=64, epochs=10,\n",
    "                     validation_data=(x_test_seq, to_categorical(y_test)), callbacks = [checkpoint], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2201,\n",
       " 2172,\n",
       " 550,\n",
       " 333,\n",
       " 776,\n",
       " 2186,\n",
       " 2343,\n",
       " 1882,\n",
       " 479,\n",
       " 2664,\n",
       " 601,\n",
       " 333,\n",
       " 789,\n",
       " 1067,\n",
       " 708,\n",
       " 1914,\n",
       " 2955,\n",
       " 898,\n",
       " 393,\n",
       " 2664,\n",
       " 2467,\n",
       " 459]"
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 1000)"
      ]
     },
     "execution_count": 785,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test), len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cnn_word2vec.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
