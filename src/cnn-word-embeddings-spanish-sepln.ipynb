{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis usando Deep Learning para espa√±ol en textos cortos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We work on a dataset of tweets labelled as positive or negative, more information about the dataset \n",
    "can be found on http://www.sepln.org/workshops/tass/. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Getting train, validation and test subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib.xmlreader as xml\n",
    "import lib.utils as ut\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import gensim.models.word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs = xml.readXML(\"../database/TASS/TASS2018/task1-Training.xml\",[0,1,2,3])\n",
    "val_docs   = xml.readXML(\"../database/TASS/TASS2018/task1-Development.xml\",[0,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets = []\n",
    "train_labels = []\n",
    "for doc in train_docs:\n",
    "    # train_tweets.append(ut.tokenize(doc.content, 0)['clean'])\n",
    "    train_tweets.append(doc.content)\n",
    "    train_labels.append(doc.polarity)\n",
    "\n",
    "val_tweets = []\n",
    "val_labels = []\n",
    "for doc in val_docs:\n",
    "    # test_tweets.append(ut.tokenize(doc.content, 0)['clean'])\n",
    "    val_tweets.append(doc.content)\n",
    "    val_labels.append(doc.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 500)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_tweets), len(val_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Sentences = 242\n",
      "       \r",
      "Negative Sentences = 231\n",
      "       \r",
      "Neutral  Sentences = 166\n",
      "       \r",
      "None Values        = 361\n"
     ]
    }
   ],
   "source": [
    "POSI_train_docs = [train_docs[i] for i in range(len(train_labels)) if train_labels[i] == 0]\n",
    "NEGA_train_docs = [train_docs[i] for i in range(len(train_labels)) if train_labels[i] == 1]\n",
    "NEUT_train_docs = [train_docs[i] for i in range(len(train_labels)) if train_labels[i] == 2]\n",
    "NONE_train_docs = [train_docs[i] for i in range(len(train_labels)) if train_labels[i] == 3]\n",
    "\n",
    "level_train_docs = [POSI_train_docs,NEGA_train_docs,NEUT_train_docs,NONE_train_docs]\n",
    "\n",
    "fmt = \"\"\"Positive Sentences = {:d}\n",
    "       \\rNegative Sentences = {:d}\n",
    "       \\rNeutral  Sentences = {:d}\n",
    "       \\rNone Values        = {:d}\"\"\"\n",
    "\n",
    "print(fmt.format(len(POSI_train_docs),\n",
    "                 len(NEGA_train_docs),\n",
    "                 len(NEUT_train_docs),\n",
    "                 len(NONE_train_docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum number of sentences per level :  166\n"
     ]
    }
   ],
   "source": [
    "minSentLvl = min(len(POSI_train_docs),len(NEGA_train_docs),len(NEUT_train_docs),len(NONE_train_docs))\n",
    "\n",
    "print('Minimum number of sentences per level : ', minSentLvl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "new_train_docs = []\n",
    "for i in range(len(level_train_docs)):\n",
    "    level_per = random.sample(level_train_docs[i],len(level_train_docs[i]))\n",
    "    new_train_docs.append(level_per[:minSentLvl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New size of sentences:\n",
      "\n",
      "Positive Sentences = 166\n",
      "       \r",
      "Negative Sentences = 166\n",
      "       \r",
      "Neutral  Sentences = 166\n",
      "       \r",
      "None Values        = 166\n"
     ]
    }
   ],
   "source": [
    "print(\"New size of sentences:\\n\")\n",
    "fmt = \"\"\"Positive Sentences = {:d}\n",
    "       \\rNegative Sentences = {:d}\n",
    "       \\rNeutral  Sentences = {:d}\n",
    "       \\rNone Values        = {:d}\"\"\"\n",
    "\n",
    "print(fmt.format(len(new_train_docs[0]),\n",
    "                 len(new_train_docs[1]),\n",
    "                 len(new_train_docs[2]),\n",
    "                 len(new_train_docs[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuf_train_docs size =  664\n"
     ]
    }
   ],
   "source": [
    "flat_train_docs = [item for sublist in new_train_docs for item in sublist]\n",
    "shuf_train_docs = random.sample(flat_train_docs,len(flat_train_docs))\n",
    "\n",
    "assert (len(shuf_train_docs) == 4 * minSentLvl)\n",
    "print(\"shuf_train_docs size = \", len(shuf_train_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for doc in shuf_train_docs + val_docs:\n",
    "    corpus.append(doc.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences =  1164\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentences = \", (len(val_docs + shuf_train_docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuf_train_labels = []\n",
    "for doc in shuf_train_docs:\n",
    "    shuf_train_labels.append(doc.polarity)\n",
    "    \n",
    "assert (len(shuf_train_labels) == len(shuf_train_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "def gensim_load_vec(path=\"../database/embeddings/SBW-vectors-300-min5.bin\"):\n",
    "    #use gensim_emb.wv.index2word if used this way to load vectors\n",
    "    #gensim_emb = gensim.models.word2vec.Word2Vec.load(path)\n",
    "    gensim_emb =  gensim.models.KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "    vocab = gensim_emb.index2word\n",
    "    vec = gensim_emb.syn0\n",
    "    shape = gensim_emb.syn0.shape\n",
    "    return gensim_emb, vec, shape, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jose/.local/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \n",
      "/home/jose/.local/lib/python3.6/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "gensim_emb, vec, shape, vocab = gensim_load_vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = CountVectorizer(tokenizer=ut.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1164, 4640)\n"
     ]
    }
   ],
   "source": [
    "X = counter.fit_transform(corpus)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4640, 300)\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = X.shape[1]\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, 300))\n",
    "for word in vocab:\n",
    "    try:\n",
    "        i = counter.vocabulary_[word]\n",
    "        embedding_matrix[i] = gensim_emb[word]\n",
    "    except KeyError:\n",
    "        pass\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(embedding_matrix[counter.vocabulary_['hola']], gensim_emb['hola'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs   = xml.readXMLTest(\"../database/TASS/TASS2018/task1-Test.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweets = []\n",
    "for doc in test_docs:\n",
    "    test_tweets.append(doc.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (len(test_tweets) == 1428)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = []\n",
    "for tweet in corpus + test_tweets:\n",
    "    sentence = []\n",
    "    for word in ut.tokenizer(tweet):\n",
    "        try:\n",
    "            i = counter.vocabulary_[word]\n",
    "            sentence.append(i)\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "    sequences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (len(sequences) == (len(shuf_train_docs) + len(val_docs) + len(test_tweets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data train tensor: (664, 45)\n",
      "Shape of data val  tensor: (500, 45)\n",
      "Shape of data test  tensor: (1428, 45)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "x_train_seq = pad_sequences(sequences[:len(shuf_train_labels)], maxlen=45)\n",
    "x_val_seq   = pad_sequences(sequences[len(shuf_train_labels):len(shuf_train_labels)+len(val_docs)], maxlen=45)\n",
    "x_test_set  = pad_sequences(sequences[(len(shuf_train_labels)+len(val_docs)):], maxlen=45)\n",
    "print('Shape of data train tensor:', x_train_seq.shape)\n",
    "print('Shape of data val  tensor:', x_val_seq.shape)\n",
    "print('Shape of data test  tensor:', x_test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 45)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 45, 300)      1392000     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 44, 100)      60100       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 43, 100)      90100       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 42, 100)      120100      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 100)          0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 100)          0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_9 (GlobalM (None, 100)          0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 300)          0           global_max_pooling1d_7[0][0]     \n",
      "                                                                 global_max_pooling1d_8[0][0]     \n",
      "                                                                 global_max_pooling1d_9[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 64)           19264       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 64)           0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 4)            260         dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 4)            0           dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,681,824\n",
      "Trainable params: 289,824\n",
      "Non-trainable params: 1,392,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras \n",
    "\n",
    "from keras import backend as K\n",
    "print(K.tensorflow_backend._get_available_gpus())\n",
    "\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=4, \\\n",
    "                        inter_op_parallelism_threads=4, \\\n",
    "                        allow_soft_placement=True,\\\n",
    "                        device_count = {'CPU' : 1, 'GPU' : 0})\n",
    "\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)\n",
    "    \n",
    "seed = 7\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "from keras.layers import Input, Dense, concatenate, Activation\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "tweet_input = Input(shape=(45,), dtype='int32')\n",
    "\n",
    "tweet_encoder   = Embedding(VOCAB_SIZE, 300, weights=[embedding_matrix], input_length=45, trainable=False)(tweet_input)\n",
    "bigram_branch   = Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "bigram_branch   = GlobalMaxPooling1D()(bigram_branch)\n",
    "trigram_branch  = Conv1D(filters=100, kernel_size=3, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "trigram_branch  = GlobalMaxPooling1D()(trigram_branch)\n",
    "fourgram_branch = Conv1D(filters=100, kernel_size=4, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "fourgram_branch = GlobalMaxPooling1D()(fourgram_branch)\n",
    "merged = concatenate([bigram_branch, trigram_branch, fourgram_branch], axis=1)\n",
    "\n",
    "merged = Dense(64, activation='relu')(merged)\n",
    "merged = Dropout(0.5)(merged)\n",
    "\n",
    "merged = Dense(4)(merged)\n",
    "output = Activation('softmax')(merged)\n",
    "model = Model(inputs=[tweet_input], outputs=[output])\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 664 samples, validate on 500 samples\n",
      "Epoch 1/10\n",
      "664/664 [==============================] - 1s 2ms/step - loss: 0.5773 - acc: 0.7481 - val_loss: 0.5558 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.75000, saving model to model/CNN_best_weights.01-0.7500.hdf5\n",
      "Epoch 2/10\n",
      "664/664 [==============================] - 1s 1ms/step - loss: 0.5429 - acc: 0.7508 - val_loss: 0.5904 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.75000\n",
      "Epoch 3/10\n",
      "664/664 [==============================] - 1s 1ms/step - loss: 0.5219 - acc: 0.7575 - val_loss: 0.5721 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.75000\n",
      "Epoch 4/10\n",
      "664/664 [==============================] - 1s 1ms/step - loss: 0.4894 - acc: 0.7684 - val_loss: 0.5223 - val_acc: 0.7510\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.75000 to 0.75100, saving model to model/CNN_best_weights.04-0.7510.hdf5\n",
      "Epoch 5/10\n",
      "664/664 [==============================] - 1s 1ms/step - loss: 0.4599 - acc: 0.7835 - val_loss: 0.5134 - val_acc: 0.7520\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.75100 to 0.75200, saving model to model/CNN_best_weights.05-0.7520.hdf5\n",
      "Epoch 6/10\n",
      "664/664 [==============================] - 1s 1ms/step - loss: 0.4112 - acc: 0.8155 - val_loss: 0.5347 - val_acc: 0.7510\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.75200\n",
      "Epoch 7/10\n",
      "664/664 [==============================] - 1s 1ms/step - loss: 0.3686 - acc: 0.8430 - val_loss: 0.5324 - val_acc: 0.7475\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.75200\n",
      "Epoch 8/10\n",
      "664/664 [==============================] - 1s 1ms/step - loss: 0.3160 - acc: 0.8727 - val_loss: 0.5368 - val_acc: 0.7520\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.75200\n",
      "Epoch 9/10\n",
      "664/664 [==============================] - 1s 1ms/step - loss: 0.2650 - acc: 0.9074 - val_loss: 0.6158 - val_acc: 0.7395\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.75200\n",
      "Epoch 10/10\n",
      "664/664 [==============================] - 1s 1ms/step - loss: 0.2164 - acc: 0.9292 - val_loss: 0.5713 - val_acc: 0.7440\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.75200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6188c2feb8>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "filepath=\"model/CNN_best_weights.{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "model.fit(x_train_seq, to_categorical(shuf_train_labels), batch_size=64, epochs=10,\n",
    "                     validation_data=(x_test_seq, to_categorical(val_labels)), callbacks = [checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s 461us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5712553024291992, 0.744]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from keras.models import load_model\n",
    "\n",
    "#loaded_CNN_model = load_model('model/CNN_best_weights.05-0.5400.hdf5')\n",
    "#loaded_CNN_model.evaluate(x=x_val_seq, y=y_validation)\n",
    "model.evaluate(x=x_val_seq, y=to_categorical(val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.models import load_model\n",
    "\n",
    "#best_model = load_model('model/CNN_best_weights.01-0.7500.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_values = np.argmax(model.predict(x_test_set), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabel(num):\n",
    "    if num == 0:\n",
    "        return 'N'\n",
    "    elif num == 1:\n",
    "        return 'P'\n",
    "    elif num == 2:\n",
    "        return 'NEU'\n",
    "    elif num == 3:\n",
    "        return 'NONE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def putTestValue(xmlFIle, out):\n",
    "    tree = ET.parse(xmlFIle)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    tweets = []\n",
    "    file = open(out,\"w\") \n",
    "    print(len(test_values))\n",
    "    for i,tweet in enumerate(root.iter('tweet')): \n",
    "        #print(i)\n",
    "        val = getLabel(test_values[i])\n",
    "        ID = tweet.find('tweetid').text\n",
    "        file.write(ID + \"\\t\" + val + \"\\n\")\n",
    "    file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_values = np.argmax(best_model.predict(x_test_set), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1428\n"
     ]
    }
   ],
   "source": [
    "putTestValue(\"../database/TASS/TASS2018/task1-Test.xml\", \"output_cnn_2018-1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cnn_word2vec.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
