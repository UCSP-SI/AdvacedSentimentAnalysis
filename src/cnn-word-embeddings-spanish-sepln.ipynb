{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis usando Deep Learning para espa√±ol en textos cortos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We work on a dataset of tweets labelled as positive or negative, more information about the dataset \n",
    "can be found on http://www.sepln.org/workshops/tass/. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Getting train, validation and test subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dapal\\AppData\\Local\\conda\\conda\\envs\\SI\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import lib.xmlreader as xml\n",
    "import lib.utils as ut\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import gensim.models.word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs= xml.readXML(\"../database/TASS/TASS2018/task1-Training.xml\")\n",
    "test_docs= xml.readXML(\"../database/TASS/TASS2018/task1-Development.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets = []\n",
    "train_labels = []\n",
    "for doc in train_docs:\n",
    "    # train_tweets.append(ut.tokenize(doc.content, 0)['clean'])\n",
    "    train_tweets.append(doc.content)\n",
    "    train_labels.append(doc.polarity)\n",
    "\n",
    "test_tweets = []\n",
    "test_labels = []\n",
    "for doc in test_docs:\n",
    "    # test_tweets.append(ut.tokenize(doc.content, 0)['clean'])\n",
    "    test_tweets.append(doc.content)\n",
    "    test_labels.append(doc.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(473, 201)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_tweets), len(test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P1CvRJmH-buq"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = (pd.Series(train_tweets), pd.Series(test_tweets), pd.Series(train_labels), pd.Series(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l5B1Mf3Q-h-o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set has total 473 entries with 51.16% negative, 48.84% positive\n",
      "Test set has total 201 entries with 52.74% negative, 47.26% positive\n"
     ]
    }
   ],
   "source": [
    "fmt1_ = \"Train set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\"\n",
    "print(fmt1_.format(len(x_train),\n",
    "      (len(x_train[y_train == 0]) / (len(x_train)*1.))*100,\n",
    "      (len(x_train[y_train == 1]) / (len(x_train)*1.))*100))\n",
    "\n",
    "fmt3_ = \"Test set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\"\n",
    "print(fmt3_.format(len(x_test),\n",
    "      (len(x_test[y_test == 0]) / (len(x_test)*1.))*100,\n",
    "      (len(x_test[y_test == 1]) / (len(x_test)*1.))*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "def gensim_load_vec(path=\"../database/embeddings/SBW-vectors-300-min5.bin\"):\n",
    "    #use gensim_emb.wv.index2word if used this way to load vectors\n",
    "    #gensim_emb = gensim.models.word2vec.Word2Vec.load(path)\n",
    "    gensim_emb =  gensim.models.KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "    vocab = gensim_emb.index2word\n",
    "    vec = gensim_emb.syn0\n",
    "    shape = gensim_emb.syn0.shape\n",
    "    return gensim_emb, vec, shape, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dapal\\AppData\\Local\\conda\\conda\\envs\\SI\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \n",
      "C:\\Users\\dapal\\AppData\\Local\\conda\\conda\\envs\\SI\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "gensim_emb, vec, shape, vocab = gensim_load_vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = CountVectorizer(tokenizer=ut.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(674, 3069)\n"
     ]
    }
   ],
   "source": [
    "X = counter.fit_transform(train_tweets + test_tweets)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3069, 300)\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = X.shape[1]\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, 300))\n",
    "for word in vocab:\n",
    "    try:\n",
    "        i = counter.vocabulary_[word]\n",
    "        embedding_matrix[i] = gensim_emb[word]\n",
    "    except KeyError:\n",
    "        pass\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(embedding_matrix[counter.vocabulary_['hola']], gensim_emb['hola'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = []\n",
    "for tweet in train_tweets+test_tweets:\n",
    "    sentence = []\n",
    "    for word in ut.tokenizer(tweet):\n",
    "        try:\n",
    "            i = counter.vocabulary_[word]\n",
    "            sentence.append(i)\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "    sequences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data train tensor: (473, 45)\n",
      "Shape of data val  tensor: (201, 45)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "x_train_seq = pad_sequences(sequences[:len(x_train)], maxlen=45)\n",
    "x_test_seq  = pad_sequences(sequences[len(x_train):], maxlen=45)\n",
    "print('Shape of data train tensor:', x_train_seq.shape)\n",
    "print('Shape of data val  tensor:', x_test_seq.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 45)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 45, 300)      920700      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 44, 10)       6010        embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 43, 10)       9010        embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 42, 10)       12010       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 10)           0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 10)           0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 10)           0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 30)           0           global_max_pooling1d_4[0][0]     \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "                                                                 global_max_pooling1d_6[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 64)           1984        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 64)           0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            260         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 4)            0           dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 949,974\n",
      "Trainable params: 29,274\n",
      "Non-trainable params: 920,700\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras \n",
    "\n",
    "from keras import backend as K\n",
    "print(K.tensorflow_backend._get_available_gpus())\n",
    "\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=4, \\\n",
    "                        inter_op_parallelism_threads=4, \\\n",
    "                        allow_soft_placement=True,\\\n",
    "                        device_count = {'CPU' : 1, 'GPU' : 0})\n",
    "\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)\n",
    "    \n",
    "seed = 7\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "from keras.layers import Input, Dense, concatenate, Activation\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "tweet_input = Input(shape=(45,), dtype='int32')\n",
    "\n",
    "tweet_encoder = Embedding(VOCAB_SIZE, 300, weights=[embedding_matrix], input_length=45, trainable=False)(tweet_input)\n",
    "bigram_branch = Conv1D(filters=10, kernel_size=2, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "bigram_branch  = GlobalMaxPooling1D()(bigram_branch)\n",
    "trigram_branch = Conv1D(filters=10, kernel_size=3, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "trigram_branch  = GlobalMaxPooling1D()(trigram_branch)\n",
    "fourgram_branch = Conv1D(filters=10, kernel_size=4, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "fourgram_branch = GlobalMaxPooling1D()(fourgram_branch)\n",
    "merged = concatenate([bigram_branch, trigram_branch, fourgram_branch], axis=1)\n",
    "\n",
    "merged = Dense(64, activation='relu')(merged)\n",
    "merged = Dropout(0.5)(merged)\n",
    "\n",
    "merged = Dense(4)(merged)\n",
    "output = Activation('softmax')(merged)\n",
    "model = Model(inputs=[tweet_input], outputs=[output])\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 473 samples, validate on 201 samples\n",
      "Epoch 1/15\n",
      "473/473 [==============================] - 0s 925us/step - loss: 0.3099 - acc: 0.9175 - val_loss: 0.5068 - val_acc: 0.7761\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.77612, saving model to model/CNN_best_weights.01-0.7761.hdf5\n",
      "Epoch 2/15\n",
      "473/473 [==============================] - 0s 991us/step - loss: 0.2705 - acc: 0.9429 - val_loss: 0.5046 - val_acc: 0.7612\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.77612\n",
      "Epoch 3/15\n",
      "473/473 [==============================] - 0s 958us/step - loss: 0.2307 - acc: 0.9535 - val_loss: 0.4891 - val_acc: 0.7861\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.77612 to 0.78607, saving model to model/CNN_best_weights.03-0.7861.hdf5\n",
      "Epoch 4/15\n",
      "473/473 [==============================] - 0s 826us/step - loss: 0.1921 - acc: 0.9641 - val_loss: 0.4838 - val_acc: 0.7910\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.78607 to 0.79104, saving model to model/CNN_best_weights.04-0.7910.hdf5\n",
      "Epoch 5/15\n",
      "473/473 [==============================] - 0s 826us/step - loss: 0.1652 - acc: 0.9683 - val_loss: 0.4765 - val_acc: 0.8010\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.79104 to 0.80100, saving model to model/CNN_best_weights.05-0.8010.hdf5\n",
      "Epoch 6/15\n",
      "473/473 [==============================] - 0s 793us/step - loss: 0.1307 - acc: 0.9852 - val_loss: 0.4759 - val_acc: 0.7960\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.80100\n",
      "Epoch 7/15\n",
      "473/473 [==============================] - 0s 892us/step - loss: 0.1101 - acc: 0.9958 - val_loss: 0.4866 - val_acc: 0.7811\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.80100\n",
      "Epoch 8/15\n",
      "473/473 [==============================] - 0s 826us/step - loss: 0.0899 - acc: 0.9915 - val_loss: 0.4793 - val_acc: 0.7761\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.80100\n",
      "Epoch 9/15\n",
      "473/473 [==============================] - 0s 826us/step - loss: 0.0800 - acc: 1.0000 - val_loss: 0.5014 - val_acc: 0.7711\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.80100\n",
      "Epoch 10/15\n",
      "473/473 [==============================] - 0s 826us/step - loss: 0.0686 - acc: 0.9937 - val_loss: 0.5150 - val_acc: 0.7562\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.80100\n",
      "Epoch 11/15\n",
      "473/473 [==============================] - 0s 826us/step - loss: 0.0570 - acc: 0.9979 - val_loss: 0.4975 - val_acc: 0.7811\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.80100\n",
      "Epoch 12/15\n",
      "473/473 [==============================] - 0s 892us/step - loss: 0.0457 - acc: 1.0000 - val_loss: 0.5248 - val_acc: 0.7711\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.80100\n",
      "Epoch 13/15\n",
      "473/473 [==============================] - 0s 793us/step - loss: 0.0416 - acc: 1.0000 - val_loss: 0.5108 - val_acc: 0.7910\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.80100\n",
      "Epoch 14/15\n",
      "473/473 [==============================] - 0s 925us/step - loss: 0.0345 - acc: 1.0000 - val_loss: 0.5217 - val_acc: 0.7910\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.80100\n",
      "Epoch 15/15\n",
      "473/473 [==============================] - 0s 925us/step - loss: 0.0257 - acc: 1.0000 - val_loss: 0.5390 - val_acc: 0.7861\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.80100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27dd11a4b70>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "filepath=\"model/CNN_best_weights.{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "model.fit(x_train_seq, y_train, batch_size=64, epochs=15,\n",
    "                     validation_data=(x_test_seq, y_test), callbacks = [checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201/201 [==============================] - 0s 233us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5226210966632141, 0.766169154377126]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from keras.models import load_model\n",
    "\n",
    "#loaded_CNN_model = load_model('model/CNN_best_weights.05-0.5400.hdf5')\n",
    "#loaded_CNN_model.evaluate(x=x_val_seq, y=y_validation)\n",
    "model.evaluate(x=x_test_seq, y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'acc']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cnn_word2vec.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
